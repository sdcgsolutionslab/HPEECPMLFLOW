{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing your machine learning lifecycle with MLFlow and HPE Ezmeral ML Ops - Lab 3\n",
    "## Build, experiment, train your model, track and compare model performance with MLflow on HPE Ezmeral ML Ops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Lab workflow**\n",
    "\n",
    "In this lab, using your local Jupyter Notebook sandbox:\n",
    "\n",
    "* You will develop, train and test your model. \n",
    "* You will test your ML algorithm with a variety of parameters to determine the model that yields to the best prediction result. \n",
    "* You will use **MLflow**, when running your machine learning code, for logging parameters, metrics and model artifacts.  \n",
    "* You will also train your model against a larger dataset using remote training engine cluster that offers larger computing resources with GPU.\n",
    "\n",
    ">**Note:** _This workshop is not intended to teach you about AI/ML model experimentation and development. \n",
    "It is intended to give a use case for data science end-to-end ML lifecycle management with MLflow on HPE Ezmeral ML Ops._\n",
    "\n",
    "#### About the Dataset\n",
    "The dataset is based on the 2019 New York City yellow cab trip data (approximately 375,000 trip records from January-June 2019). The dataset has many different properties (aka _\"X - features\"_) like the pickup time and location, the dropoff time and location, the trip distance, the number of passengers, and several other variables. The goal is to predict the taxi ride duration in NY City (the target aka _\"Y - label\"_) based on these features. \n",
    "\n",
    ">**Note:** _For this workshop, you will be using a premade subset of this dataset that requires little data preprocessing and preparation (i.e.: data cleaning, data transformation)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1- Code versioning integrated with Jupyter Notebook cluster**\n",
    "\n",
    "As you can see in the left hand side, as a data scientist, you have all the files (for example notebooks and code scripts) you need to do your work. These files are all pulled from the GitHub source control repository set up by the Operations team for the data science team.\n",
    "\n",
    "The Jupyter Notebook ML Ops application includes the **Git** plugin that allows data scientists to use Git and do version control of their notebooks and model code scripts directly from within their local Jupyter Notebook.\n",
    "\n",
    "From within the local Jupyter Notebook, data scientists can do the usual _git status_, _git add_, _git commit_, _git push_ to push their notebooks to their GitHub repository branch from the _**Notebook terminal**_ provided, and start versioning their notebooks and codes, and collaborate across projects.\n",
    "If there are other changes that are pushed by other data scientists in the same GitHub repository branch, user can pull up the changes from the terminal using _git pull_ command. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Launcher-terminal.jpg\" height=\"600\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn more about Git\n",
    "\n",
    "Want to learn more about Git, check out the community blog series and the HPE DEV workshop-on-demand:\n",
    "\n",
    "* [Getting started with Git - blog series](https://developer.hpe.com/blog/get-involved-in-the-open-source-community-part-1-getting-started-with-git/)\n",
    "* [Git 101 - Get involved in the open source community - Workshop-on-demand](https://hackshack.hpedev.io/workshops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">**Note: While executing code cells, you might see a \"deprecation\" warning message - please ignore it.**</font>\n",
    "\n",
    "_/opt/miniconda/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2- Checking Dataset is accessible**\n",
    "\n",
    "The Python code here is used to verify the Dataset is accessible from your local Jupyter Notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userID = \"student{{ STDID }}\"\n",
    "#smalldemodataset = True\n",
    "tinydemodataset = True\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "# Shared dataset across all tenant users, use case directory name and scoring file name\n",
    "if tinydemodataset is True:\n",
    "    datasetFile = \"demodata-tiny.csv\"\n",
    "else:\n",
    "    datasetFile = \"demodata-small.csv\"\n",
    "    \n",
    "locationTable = \"lookup-ipyheader.csv\"\n",
    "usecaseDirectory = \"NYCTaxi\"\n",
    "\n",
    "# Project repo path function - file system mount available to all app containers\n",
    "def ProjectRepo(path):\n",
    "    ProjectRepo = \"/bd-fs-mnt/TenantShare/repo\"\n",
    "    return str(ProjectRepo + '/' + path)\n",
    "\n",
    "print (\"Verifying the Dataset is accessible:\")\n",
    "\n",
    "# Making sure the input Dataset is loaded and accessible in the shared persistent container storage for your tenant\n",
    "# Check the dataset file exists in /db-fs-mnt/TenantShare/repo/data/NYCTaxi folder:\n",
    "#print (os.listdir(ProjectRepo('data/NYCTaxi')))\n",
    "datasetFilePath = \"data\" + '/' + usecaseDirectory + '/' + datasetFile\n",
    "locationFilePath = \"data\" + '/' + usecaseDirectory + '/' + locationTable\n",
    "pathData = ProjectRepo(\"data\" + '/' + usecaseDirectory)\n",
    "\n",
    "if (not os.path.exists(ProjectRepo(datasetFilePath))):\n",
    "    print (\"Error! Dataset file \" + ProjectRepo(datasetFilePath) + \" does not exist. Please contact the workshop administrator at hpedev.hackshack@hpe.com before continuing.\")\n",
    "else:\n",
    "    print (\"Demo dataset is \" + ProjectRepo(datasetFilePath))\n",
    "    \n",
    "if (not os.path.exists(ProjectRepo(locationFilePath))):\n",
    "    print (\"Error! location table file \" + ProjectRepo(locationFilePath) + \" does not exist. Please contact the workshop administrator at hpedev.hackshack@hpe.com before continuing.\")\n",
    "else:\n",
    "    print (\"Location table file is \" + ProjectRepo(locationFilePath))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3- Develop the model on local Jupyter, log and track your experiments on HPE Ezmeral ML Ops with MLflow**\n",
    "\n",
    "In general, data scientists use their local Jupyter Notebook for **experimenting** several learning algorithms with a variety of parameters. They do so to determine the ML model that works best for the business problem they try to address and develop the model that yields to the best prediction result. Then, within their notebooks, they submit their code to large scaled computing training cluster environment to train and test their full ML models, in a reasonable time, typically against a larger training dataset and test dataset. The output of this step is a trained model ready for deployment in production.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About the machine learning model development workflow\n",
    "Gradient boosting supervised machine learning algorithm implementation in the **scikit-learn (sklearn) library for Python** is used here to build the model that is capable of predicting the duration of taxi trips in New York city. Python libraries such as Numpy, Pandas, Scikit-learn, XGBoost are used to build the model. \n",
    "\n",
    "The machine learning workflow depicted in the diagram below follows the typical supervised machine learning workflow for models development:\n",
    "\n",
    "<img src=\"ML-Workflow.jpg\" height=\"600\" width=\"600\" align=\"right\">\n",
    "\n",
    "- After loading the dataset (historical data) from the central project repository, the ML algorithm separates data into features (the taxi ride properties) and label (the taxi ride duration). \n",
    "- Then the dataset is divided into two parts, one for training the model and one for testing the model. The typical split is 80/20.\n",
    "- The ML algorithm is then defined and the model is built with the training data set to learn from. \n",
    "- The resulting model is then run on the test data which was not used to train the model.\n",
    "- Next, the model accuracy is evaluated by comparing the test predictions to the test labels. Error metrics such as RMSE are used here to evaluate the predictions accuracy.\n",
    "- The trained model is finally ***saved as an artifact on the MLflow tracking server***. The model is ready for deployment in production to serve predictions.\n",
    "- The next step is then to move your trained model to production to serve your model as a prediction service (Lab Part 4) and make predictions on new data points (Lab Part 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary packages and library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Importing libraries\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import math\n",
    "import os\n",
    "import datetime\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tinydemodataset = True\n",
    "usecaseDirectory = \"NYCTaxi\"\n",
    "\n",
    "if tinydemodataset is True:\n",
    "    datasetFile = \"demodata-tiny.csv\"\n",
    "else:\n",
    "    datasetFile = \"demodata-small.csv\"\n",
    "    \n",
    "locationTable = \"lookup-ipyheader.csv\"\n",
    "datasetFilePath = \"data\" + '/' + usecaseDirectory + '/' + datasetFile\n",
    "locationFilePath = \"data\" + '/' + usecaseDirectory + '/' + locationTable\n",
    "\n",
    "# Start time \n",
    "print(\"Start time for \" + userID + \": \", datetime.datetime.now())\n",
    "\n",
    "# Project repo path function\n",
    "def ProjectRepo(path):\n",
    "    ProjectRepo = \"/bd-fs-mnt/TenantShare/repo\"\n",
    "    return str(ProjectRepo + '/' + path)\n",
    "\n",
    "print(\"Reading in data\")\n",
    "# Reading in dataset table using pandas\n",
    "dbName = \"pqyellowtaxi\"\n",
    "##df = pd.read_csv(ProjectRepo('data/NYCTaxi/demodata.csv'))\n",
    "try:\n",
    "    df = pd.read_csv(ProjectRepo(datasetFilePath))\n",
    "except Exception as e:\n",
    "    logger.exception(\n",
    "        \"Unable to read the dataset, check with your administrator. Error: %s\", e\n",
    "        )\n",
    "\n",
    "# Reading in latitude/longitude coordinate lookup table using pandas \n",
    "lookupDbName = \"pqlookup\"\n",
    "##dflook = pd.read_csv(ProjectRepo('data/NYCTaxi/lookup-ipyheader.csv'))\n",
    "try:\n",
    "    dflook = pd.read_csv(ProjectRepo(locationFilePath))\n",
    "except Exception as e:\n",
    "    logger.exception(\n",
    "        \"Unable to read the location table, check with your administrator. Error: %s\", e\n",
    "        )\n",
    "    \n",
    "print(\"Done reading in data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflook.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing and preparation (data cleaning, data transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Data preparation\")\n",
    "# merging dataset and lookup tables on latitudes/coordinates\n",
    "df = pd.merge(df, dflook[[lookupDbName + '.location_i', lookupDbName + '.long', lookupDbName + '.lat']], how='left', left_on=dbName + '.pulocationid', right_on=lookupDbName + '.location_i')\n",
    "df.rename(columns = {(lookupDbName + '.long'):(dbName + '.startstationlongitude')}, inplace = True)\n",
    "df.rename(columns = {(lookupDbName + '.lat'):(dbName + '.startstationlatitude')}, inplace = True)\n",
    "df = pd.merge(df, dflook[[lookupDbName + '.location_i', lookupDbName + '.long', lookupDbName + '.lat']], how='left', left_on=dbName + '.dolocationid', right_on=lookupDbName + '.location_i')\n",
    "df.rename(columns = {(lookupDbName + '.long'):(dbName + '.endstationlongitude')}, inplace = True)\n",
    "df.rename(columns = {(lookupDbName + '.lat'):(dbName + '.endstationlatitude')}, inplace = True)\n",
    "\n",
    "\n",
    "def fullName(colName):\n",
    "    return dbName + '.' + colName\n",
    "\n",
    "# convert string to datetime\n",
    "df[fullName('tpep_pickup_datetime')] = pd.to_datetime(df[fullName('tpep_pickup_datetime')])\n",
    "df[fullName('tpep_dropoff_datetime')] = pd.to_datetime(df[fullName('tpep_dropoff_datetime')])\n",
    "df[fullName('duration')] = (df[fullName(\"tpep_dropoff_datetime\")] - df[fullName(\"tpep_pickup_datetime\")]).dt.total_seconds()\n",
    "\n",
    "# feature engineering\n",
    "# Feature engineering is the process of transforming raw data into inputs for a machine learning algorithm\n",
    "df[fullName(\"weekday\")] = (df[fullName('tpep_pickup_datetime')].dt.dayofweek < 5).astype(float)\n",
    "df[fullName(\"hour\")] = df[fullName('tpep_pickup_datetime')].dt.hour\n",
    "df[fullName(\"work\")] = (df[fullName('weekday')] == 1) & (df[fullName(\"hour\")] >= 8) & (df[fullName(\"hour\")] < 18)\n",
    "df[fullName(\"month\")] = df[fullName('tpep_pickup_datetime')].dt.month\n",
    "# convert month to a categorical feature using one-hot encoding\n",
    "df = pd.get_dummies(df, columns=[fullName(\"month\")])\n",
    "\n",
    "# Filter dataset to rides under 3 hours and under 150 miles to remove outliers\n",
    "df = df[df[fullName('duration')] > 20]\n",
    "df = df[df[fullName('duration')] < 10800]\n",
    "df = df[df[fullName('trip_distance')] > 0]\n",
    "df = df[df[fullName('trip_distance')] < 150]\n",
    "\n",
    "# drop null rows\n",
    "df = df.dropna(how='any',axis=0)\n",
    "\n",
    "# select columns to be used as features\n",
    "cols = [fullName('work'), fullName('startstationlatitude'), fullName('startstationlongitude'), fullName('endstationlatitude'), fullName('endstationlongitude'), fullName('trip_distance'), fullName('weekday'), fullName('hour')]\n",
    "cols.extend([fullName('month_' + str(x)) for x in range(1, 7)])\n",
    "cols.append(fullName('duration'))\n",
    "dataset = df[cols]\n",
    "\n",
    "# separate data into features (the taxi ride properties) and label (duration) using .iloc\n",
    "X = dataset.iloc[:, 0:(len(cols) - 1)].values\n",
    "y = dataset.iloc[:, (len(cols) - 1)].values\n",
    "X = X.copy()\n",
    "y = y.copy()\n",
    "print (X.shape)\n",
    "print (y.shape)\n",
    "\n",
    "#del dataset\n",
    "#del df\n",
    "\n",
    "print(\"Done preparing data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Take a quick glance at our prepared dataset. These are the columns (features) that we will be working with. Our target variable is the \"duration\" column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test your model using your local Jupyter Notebook sandbox.\n",
    "Execute the Notebook code cell below to train and test your model on your local Jupyter Notebook sandbox. \n",
    "\n",
    "Let the code cell run until completion. This should take a couple of minutes to complete the trainign of your model. You will see the message in the output of the code cell: **Training and testing of your model finished!** \n",
    "\n",
    "> **Note:** When executing a notebook code cell, a [*] next to the action, it means the execution step is busy working within the notebook.   \n",
    "A [digit number] next to the action means the execution step has completed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start the training and testing of your model...\")\n",
    "\n",
    "# As we have one dataset, the data is split into a training data set and a test data set. The ideal split is 80:20. \n",
    "# 80% of the data is used to train the model and 20% is used for testing the model.\n",
    "print(\"Load the sklearn module to split the dataset into a training data set and a test data set.\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "print(\"Build the model and fit the model on the training data. This will take a few minutes...\")\n",
    "\n",
    "# Define the ML algorithm (that is the learning algorithm to use). \n",
    "# We use the gradient boosting algorithm implementation in the scikit-learn (sklearn) library.\n",
    "# We use here the XGBRegressor class of the xgboost package via the scikit-learn wrapper classes \n",
    "# to build the model.\n",
    "# The hyperparameter tree_method is used to enable training of an XGBoost model using the GPU device. \n",
    "xgbr = xgb.XGBRegressor(objective ='reg:squarederror',\n",
    "                        colsample_bytree = 1,\n",
    "                        subsample = 1,\n",
    "                        learning_rate = 0.15,\n",
    "                        booster = \"gbtree\",\n",
    "                        max_depth = 1,\n",
    "                        eta = 0.5,\n",
    "                        eval_metric = \"rmse\",) \n",
    "\n",
    "numtrainelements = str(len(X_train))\n",
    "#print(\"num train elements: \" + str(len(X_train)))\n",
    "print(\"number of train elements: \" + numtrainelements)\n",
    "\n",
    "# then train (fit) the model on training data set comprised of inputs (features) and outputs (label)\n",
    "# we provide our defined ML algorithm with data to learn from\n",
    "print(\"Train start time: \", datetime.datetime.now())\n",
    "tmp = datetime.datetime.now()\n",
    "xgbr.fit(X_train, y_train)\n",
    "#after training the model, check the model training score. The closer towards 1, the better the fit.\n",
    "score = xgbr.score(X_train, y_train)  \n",
    "print(\"Model training score. The closer towards 1, the better the fit: \", score)\n",
    "\n",
    "print(\"Train end time: \", datetime.datetime.now())\n",
    "cpu_time = datetime.datetime.now() - tmp\n",
    "print (\"Training time on local Jupyter Notebook: %s seconds\" %(str(cpu_time)))\n",
    "print(\"Test the model by making predictions on the test data set where only the features are provided\")\n",
    "y_pred = xgbr.predict(X_test)\n",
    "y_pred = y_pred.clip(min=0)\n",
    "\n",
    "print(\"Evaluating the model accuracy by comparing the test predictions with the test labels. RMSE is used as evaluation metric.\")\n",
    "# evaluating the model accuracy\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "mae = metrics.mean_absolute_error(y_test, y_pred)\n",
    "mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "rmsle = np.sqrt(mean_squared_log_error( y_test, y_pred))\n",
    "\n",
    "print(\"  Root Mean Squared Error - the lower the value is, the better the fit: %s\" % rmse)\n",
    "print(\"  Mean Absolute Error: %s\" % mae)\n",
    "print(\"  Mean Squared Error: %s\" % mse)\n",
    "print(\"  Root Mean Squared Log Error: %s\" % rmsle)\n",
    "print()\n",
    "print(\"Note that for RMSE the lower the value is, the better the fit\")\n",
    "print(\"\")\n",
    "print(\"Training and testing of your model finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log and track your model performance with MLflow on HPE Ezmeral ML Ops.\n",
    "\n",
    "The operations team has deployed an _MLflow tracking server_ instance on HPE Ezmeral ML Ops, with an embedded MinIO S3 store and a SQL database for storing the model metadata (the collection of parameters, metrics, tags, and model artifact URI) and model artifacts (model files) related to the training process of your ML model. You can use the **MLflow Tracking Python API** to log data such as parameters, metrics, tags and models artifacts to the MLflow tracking server when running your ML code and to query it.\n",
    "\n",
    "MLflow tracking is organized around the concept of ***runs***, which are individual execution of some piece of data science code. And you typically organize `runs` into ***experiments*** to group runs of the same business problem you want to address. \n",
    "\n",
    "You need to tell your Python code script which MLflow tracking server URI and Experiment name to use to log the `runs`. You will do this using the Jupyter Notebook custom ***magic*** functions ***%loadMlflow*** and ***%Setexp*** respectively.\n",
    "\n",
    ">**Note:** _The Jupyter Notebook ML Ops application used in our solution includes **custom magic** functions to log and track parameters, metrics and models to a remote MLflow tracking server._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the MLflow environment variables\n",
    "Set the MLflow tracking server URI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%loadMlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this workshop, you set an MLflow expirement for your project, named `student<Id>-NYCTaxi-Experiment` using the custom magic command ***%Setexp***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimentName = userID + \"-NYCTaxi-Experiment\"\n",
    "print (\"Experiment name is: \" + experimentName )\n",
    "\n",
    "%Setexp --name $experimentName\n",
    "\n",
    "#replaced the two lines below:\n",
    "# mlflow.set_experiment('experimentName') - set the given experiment as the \"active\" experiment into which MLflow 'runs' are grouped/organized.\n",
    "#            A new experiment is created if it does not exist. it also start a new MLflow run and set it as the active run.\n",
    "# mlflow.set_tag('mlflow.user','studentxxx') - set a tag under the current run. \n",
    "#           where mlflow.user is the identifier of the user who created the run.\n",
    "\n",
    "print (\"Set the experiment \" + experimentName + \" as active experiment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launch the training job runs for your experiment\n",
    "\n",
    "For each training job run, the scikit-learn (sklearn) implementation for Python of the ML algorithm records a new `run` in MLflow tracking server to keep track of metadata information (input parameters, metrics, tags, model artifact URI) and model artifacts (model files) of the generated ML model.\n",
    "\n",
    "The following API calls are used to start and manage MLflow runs:\n",
    "\n",
    "* **start_run()** – Starts a new MLflow run, setting it as the active run under which metrics and parameters are logged.\n",
    "* **log_params()** – Logs a parameter under the current run.\n",
    "* **log_metric()** – Logs a metric under the current run.\n",
    "* **set_tags()** - Logs a set of tags under the current run.\n",
    "* **sklearn.log_model()** – Logs a Scikit-learn model as an MLflow artifact for the current run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow import log_metric, log_param, log_artifact\n",
    "\n",
    "#display the tracking server URI\n",
    "artifact_uri = mlflow.get_tracking_uri()\n",
    "print(\"Artifact uri: {}\".format(artifact_uri))\n",
    "\n",
    "#get information about the experiment\n",
    "experimentName = userID + \"-NYCTaxi-Experiment\"\n",
    "experiment_name = mlflow.get_experiment_by_name(experimentName)\n",
    "TeamExperimentId=experiment_name.experiment_id\n",
    "print(\"Experiment_id: {}\".format(experiment_name.experiment_id))\n",
    "print(\"Name: {}\".format(experiment_name.name))\n",
    "print(\"Artifact Location: {}\".format(experiment_name.artifact_location))\n",
    "print(\"Tags: {}\".format(experiment_name.tags))\n",
    "print(\"Lifecycle_stage: {}\".format(experiment_name.lifecycle_stage))\n",
    "#\n",
    "print(\"Your team Experiment Id is: \",TeamExperimentId)\n",
    "\n",
    "tags = {\"team\": userID,\n",
    "        \"dataset\": \"Tiny\",\n",
    "        \"Algo-Library\": \"XGBoost-sklearn\",\n",
    "        \"Processor\": \"CPU\"}\n",
    "\n",
    "# mlflow: stop active runs if any\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "    \n",
    "#Start a new MLflow run, setting it as the active run under which metrics, parameters and model artifacts (model files) will be logged\n",
    "print(\"Start a new MLflow run, log ML algorithm parameters, training and test metrics:\")\n",
    "print (\"\")\n",
    "\n",
    "MyRunName = \"Run-Test-1\"\n",
    "mlflow.start_run(experiment_id=TeamExperimentId,run_name=MyRunName)\n",
    "mlflow.log_param(\"learning_rate\", 0.15)\n",
    "mlflow.log_param(\"max_depth\", 1)\n",
    "mlflow.log_metric(\"tr_score\", score)\n",
    "mlflow.log_metric(\"eval_rmse\", rmse)\n",
    "mlflow.set_tags(tags)\n",
    "\n",
    "# Log the scikit-learn model as an MLflow artifact for the current run.\n",
    "mlflow.sklearn.log_model(xgbr, \"model\")\n",
    "print(\"Model logged as an MLflow artifact for your current run \" + MyRunName)\n",
    "\n",
    "# mlflow: end tracking\n",
    "mlflow.end_run()\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4- Train the model on the remote shared training cluster**\n",
    "\n",
    "In general, data scientists use their local Jupyter Notebook for **experimenting** several learning algorithms with a variety of parameters. They do so to determine the ML model that works best for the business problem they try to address and develop the model that yields to the best prediction result. Then, within their notebooks, they submit their code to large scaled computing training cluster environment to train and test their full ML models, in a reasonable time, typically against a larger training dataset and test dataset. The output of this step is a trained model ready for deployment in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Connection to the remote tenant-shared training cluster\n",
    "\n",
    "Next, before training your model on remote shared training cluster, you may want to test that the communication with the shared training cluster is indeed functioning properly.\n",
    "\n",
    "> **Note:** _The Jupyter Notebook ML Ops application used in our solution includes **custom magic** functions to handle remotely submitting training code and retrieving results and logs. The Notebook uses these magic functions to make REST API calls to the API server that runs as part of the shared training environment. These calls submit training jobs and get results from within the Notebook session._\n",
    "\n",
    "The Jupyter Notebook ML Ops app includes the following **line magic** functions: \n",
    "*\t%attachments: Returns a list of connected training environments. \n",
    "*\t%logs --url: URL of the training server load balancer used to monitor the status of the training job.\n",
    "\n",
    "The Jupyter Notebook ML Ops app also includes the following **cell magic** function:\n",
    "*\t%%training_cluster_name \n",
    "This would submit training code to the shared training environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the list of connected training environments\n",
    "\n",
    "<b>%attachments</b> is a line magic command that output a table with the name(s) of the training cluster(s) available for us to use. Sometimes, Operations team may have created multiple training clusters for different projects depending on the needs of the model or size of data, e.g. some with GPU nodes, while others with CPUs only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%attachments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit a notebook code cell to a remote training cluster\n",
    "\n",
    "To utilize the training cluster, you will need grab the name of the training cluster you want to use and feed it into another custom cell magic command. \n",
    "\n",
    "With the **%%trainingengineshared** magic command specified at the beginning of the code cell, the Jupyter Notebook will \n",
    "submit the entire content of the cell to the training cluster named _trainingengineshared_. If you comment this magic command, the code will run on your local Jupyter Notebook.\n",
    "\n",
    "The example cell below will execute a print statement on the training cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture history_url\n",
    "\n",
    "%%trainingengineshared\n",
    "\n",
    "import datetime\n",
    "\n",
    "print('test')\n",
    "print(\"Date time: \", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve the result of the job\n",
    "\n",
    "The training cluster will send back a unique log url for the job submitted.   \n",
    "You can use the _History URL_ with the _\"%log --url\"_ custom **line magic** command to track the status of the job in real time. \n",
    "\n",
    "A status of \"**Finished**\" means the execution of the job submitted to the training cluster has completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyurl = history_url.stdout.split(' ')[2]\n",
    "print(historyurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%logs --url $historyurl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's submit the Python code to remote training engine cluster (with GPU) while logging and tracking parameters, metrics and models with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture history_url\n",
    "\n",
    "%%trainingengineshared\n",
    "\n",
    "userID = \"student{{ STDID }}\"\n",
    "smalldemodataset = True\n",
    "\n",
    "print(\"Importing libraries\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import math\n",
    "import os\n",
    "import datetime\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow import log_metric, log_param, log_artifact\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "usecaseDirectory = \"NYCTaxi\"\n",
    "\n",
    "if smalldemodataset is True:\n",
    "    datasetFile = \"demodata-small.csv\"\n",
    "else:\n",
    "    datasetFile = \"demodata.csv\"\n",
    "    \n",
    "locationTable = \"lookup-ipyheader.csv\"\n",
    "datasetFilePath = \"data\" + '/' + usecaseDirectory + '/' + datasetFile\n",
    "locationFilePath = \"data\" + '/' + usecaseDirectory + '/' + locationTable\n",
    "studentRepoModel = \"models\" + '/' + usecaseDirectory + '/' + userID\n",
    "\n",
    "# Start time \n",
    "print(\"Start time for \" + userID + \": \", datetime.datetime.now())\n",
    "\n",
    "# Project repo path function\n",
    "def ProjectRepo(path):\n",
    "    ProjectRepo = \"/bd-fs-mnt/TenantShare/repo\"\n",
    "    return str(ProjectRepo + '/' + path)\n",
    "\n",
    "\n",
    "print(\"Reading in data\")\n",
    "# Reading in dataset table using pandas\n",
    "dbName = \"pqyellowtaxi\"\n",
    "##df = pd.read_csv(ProjectRepo('data/NYCTaxi/demodata.csv'))\n",
    "df = pd.read_csv(ProjectRepo(datasetFilePath))\n",
    "\n",
    "# Reading in latitude/longitude coordinate lookup table using pandas \n",
    "lookupDbName = \"pqlookup\"\n",
    "##dflook = pd.read_csv(ProjectRepo('data/NYCTaxi/lookup-ipyheader.csv'))\n",
    "dflook = pd.read_csv(ProjectRepo(locationFilePath))\n",
    "print(\"Done reading in data\")\n",
    "\n",
    "\n",
    "# merging dataset and lookup tables on latitudes/coordinates\n",
    "df = pd.merge(df, dflook[[lookupDbName + '.location_i', lookupDbName + '.long', lookupDbName + '.lat']], how='left', left_on=dbName + '.pulocationid', right_on=lookupDbName + '.location_i')\n",
    "df.rename(columns = {(lookupDbName + '.long'):(dbName + '.startstationlongitude')}, inplace = True)\n",
    "df.rename(columns = {(lookupDbName + '.lat'):(dbName + '.startstationlatitude')}, inplace = True)\n",
    "df = pd.merge(df, dflook[[lookupDbName + '.location_i', lookupDbName + '.long', lookupDbName + '.lat']], how='left', left_on=dbName + '.dolocationid', right_on=lookupDbName + '.location_i')\n",
    "df.rename(columns = {(lookupDbName + '.long'):(dbName + '.endstationlongitude')}, inplace = True)\n",
    "df.rename(columns = {(lookupDbName + '.lat'):(dbName + '.endstationlatitude')}, inplace = True)\n",
    "\n",
    "\n",
    "def fullName(colName):\n",
    "    return dbName + '.' + colName\n",
    "\n",
    "# convert string to datetime\n",
    "df[fullName('tpep_pickup_datetime')] = pd.to_datetime(df[fullName('tpep_pickup_datetime')])\n",
    "df[fullName('tpep_dropoff_datetime')] = pd.to_datetime(df[fullName('tpep_dropoff_datetime')])\n",
    "df[fullName('duration')] = (df[fullName(\"tpep_dropoff_datetime\")] - df[fullName(\"tpep_pickup_datetime\")]).dt.total_seconds()\n",
    "\n",
    "# feature engineering\n",
    "# Feature engineering is the process of transforming raw data into inputs for a machine learning algorithm\n",
    "df[fullName(\"weekday\")] = (df[fullName('tpep_pickup_datetime')].dt.dayofweek < 5).astype(float)\n",
    "df[fullName(\"hour\")] = df[fullName('tpep_pickup_datetime')].dt.hour\n",
    "df[fullName(\"work\")] = (df[fullName('weekday')] == 1) & (df[fullName(\"hour\")] >= 8) & (df[fullName(\"hour\")] < 18)\n",
    "df[fullName(\"month\")] = df[fullName('tpep_pickup_datetime')].dt.month\n",
    "# convert month to a categorical feature using one-hot encoding\n",
    "df = pd.get_dummies(df, columns=[fullName(\"month\")])\n",
    "\n",
    "# Filter dataset to rides under 3 hours and under 150 miles to remove outliers\n",
    "df = df[df[fullName('duration')] > 20]\n",
    "df = df[df[fullName('duration')] < 10800]\n",
    "df = df[df[fullName('trip_distance')] > 0]\n",
    "df = df[df[fullName('trip_distance')] < 150]\n",
    "\n",
    "# drop null rows\n",
    "df = df.dropna(how='any',axis=0)\n",
    "\n",
    "# select columns to be used as features\n",
    "cols = [fullName('work'), fullName('startstationlatitude'), fullName('startstationlongitude'), fullName('endstationlatitude'), fullName('endstationlongitude'), fullName('trip_distance'), fullName('weekday'), fullName('hour')]\n",
    "cols.extend([fullName('month_' + str(x)) for x in range(1, 7)])\n",
    "cols.append(fullName('duration'))\n",
    "dataset = df[cols]\n",
    "\n",
    "# separate data into features (the taxi ride properties) and label (duration) using .iloc\n",
    "X = dataset.iloc[:, 0:(len(cols) - 1)].values\n",
    "y = dataset.iloc[:, (len(cols) - 1)].values\n",
    "X = X.copy()\n",
    "y = y.copy()\n",
    "print (X.shape)\n",
    "print (y.shape)\n",
    "del dataset\n",
    "del df\n",
    "\n",
    "print(\"Done cleaning data\")\n",
    "\n",
    "\n",
    "print(\"Training and testing...\")\n",
    "\n",
    "# As we have one dataset, the data is split into a training data set and a test data set. The ideal split is 80:20. \n",
    "# 80% of the data is used to train the model and 20% is used for testing the model.\n",
    "print(\"Load the sklearn module to split the dataset into a training data set and a test data set.\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "print(\"Build the model and fit the model on the training data. This will take a few seconds on GPU based server...\")\n",
    "\n",
    "# Define the ML algorithm (that is the learning algorithm to use). \n",
    "# We use the gradient boosting algorithm implementation in the scikit-learn (sklearn) library.\n",
    "# We use here the XGBRegressor class of the xgboost package via the scikit-learn wrapper classes \n",
    "# to build the model.\n",
    "# The hyperparameter tree_method is used to enable training of an XGBoost model using the GPU device. \n",
    "xgbr = xgb.XGBRegressor(objective ='reg:squarederror',\n",
    "                        tree_method = 'gpu_hist',\n",
    "                        colsample_bytree = 1,\n",
    "                        subsample = 1,\n",
    "                        learning_rate = 0.15,\n",
    "                        booster = \"gbtree\",\n",
    "                        max_depth = 3,\n",
    "                        eta = 0.5,\n",
    "                        eval_metric = \"rmse\",) \n",
    "\n",
    "print(\"num train elements: \" + str(len(X_train)))\n",
    "\n",
    "# then train (fit) the model on training data set comprised of inputs (features) and outputs (label)\n",
    "# we provide our defined ML algorithm with data to learn from\n",
    "print(\"Train start time: \", datetime.datetime.now())\n",
    "tmp = datetime.datetime.now()\n",
    "xgbr.fit(X_train, y_train)\n",
    "#after training the model, check the model training score. The closer towards 1, the better the fit.\n",
    "score = xgbr.score(X_train, y_train)  \n",
    "print(\"Model training score. The closer towards 1, the better the fit: \", score)\n",
    "\n",
    "print(\"Train end time: \", datetime.datetime.now())\n",
    "gpu_time = datetime.datetime.now() - tmp\n",
    "print (\"GPU Training time: %s seconds\" %(str(gpu_time)))\n",
    "print(\"Test the model by making predictions on the test data set where only the features are provided\")\n",
    "y_pred = xgbr.predict(X_test)\n",
    "y_pred = y_pred.clip(min=0)\n",
    "\n",
    "print(\"Evaluating the model accuracy by comparing the test predictions with the test labels. RMSE is used as evaluation metric.\")\n",
    "# evaluating the model accuracy\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "mae = metrics.mean_absolute_error(y_test, y_pred)\n",
    "mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "rmsle = np.sqrt(mean_squared_log_error( y_test, y_pred))\n",
    "\n",
    "print(\"  Root Mean Squared Error - the lower the value is, the better the fit: %s\" % rmse)\n",
    "print(\"  Mean Absolute Error: %s\" % mae)\n",
    "print(\"  Mean Squared Error: %s\" % mse)\n",
    "print(\"  Root Mean Squared Log Error: %s\" % rmsle)\n",
    "print()\n",
    "print(\"Note that for RMSE the lower the value is, the better the fit\")\n",
    "\n",
    "#display the tracking server URI\n",
    "artifact_uri = mlflow.get_tracking_uri()\n",
    "print(\"Artifact uri: {}\".format(artifact_uri))\n",
    "\n",
    "#get information about the experiment\n",
    "experimentName = userID + \"-NYCTaxi-Experiment\"\n",
    "print (\"Experiment name is: \" + experimentName )\n",
    "experiment_name = mlflow.get_experiment_by_name(experimentName)\n",
    "TeamExperimentId=experiment_name.experiment_id\n",
    "print(\"Experiment_id: {}\".format(experiment_name.experiment_id))\n",
    "print(\"Name: {}\".format(experiment_name.name))\n",
    "print(\"Artifact Location: {}\".format(experiment_name.artifact_location))\n",
    "print(\"Tags: {}\".format(experiment_name.tags))\n",
    "print(\"Lifecycle_stage: {}\".format(experiment_name.lifecycle_stage))\n",
    "#\n",
    "print(\"Your team Experiment Id is: \",TeamExperimentId)\n",
    "\n",
    "tags = {\"team\": userID,\n",
    "        \"dataset\": \"Small\",\n",
    "        \"Algo-Library\": \"XGBoost-sklearn\",\n",
    "        \"Processor\": \"GPU\"}\n",
    "\n",
    "# mlflow: stop active runs if any\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "    \n",
    "#Start a new MLflow run, setting it as the active run under which metrics, parameters and model artifacts (model files) will be logged\n",
    "print(\"Start a new MLflow run, log ML algorithm parameters, training and test metrics\")\n",
    "print (\"\")\n",
    "\n",
    "MyRunName = \"Run-Training-Remote\"\n",
    "mlflow.start_run(experiment_id=TeamExperimentId,run_name=MyRunName)\n",
    "mlflow.log_param(\"learning_rate\", 0.15)\n",
    "mlflow.log_param(\"max_depth\", 3)\n",
    "mlflow.log_metric(\"tr_score\", score)\n",
    "mlflow.log_metric(\"eval_rmse\", rmse)\n",
    "mlflow.set_tags(tags)\n",
    "\n",
    "# Log the scikit-learn model as an MLflow artifact for the current run.\n",
    "mlflow.sklearn.log_model(xgbr, \"model\")\n",
    "print(\"Model logged as an MLflow artifact for your current run \" + MyRunName)\n",
    "\n",
    "# mlflow: end tracking\n",
    "mlflow.end_run()\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Finish time\n",
    "print(\"End time: \", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitor the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyurl = history_url.stdout.split(' ')[2]\n",
    "print(historyurl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run the cell code below regularly until the job status is marked as \"**Finished**\"\n",
    "\n",
    ">**Note:** Depending on the number of concurrent jobs submitted to the training cluster environment (i.e: multiple participants run the workshop concurrently), the model training may take several minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%logs --url $historyurl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5- Using the MLflow server UI to evaluate how the model performed** \n",
    "\n",
    "#### Navigate to the MLflow tracking server UI \n",
    "\n",
    "You can use the MLflow tracking server UI to visualize, compare and search `runs` by Experiments. \n",
    "\n",
    "From your browser, go to the MLflow tracking server UI and make sure you refresh your navigator tab to update the MLflow UI page. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Identify the `runs` for your Experiment\n",
    "\n",
    "Select the Experiment name for your project: **\\<StudentId\\>-NYCTaxi-Experiment** on the left pane to visualize the associated set of `runs` grouped under your Experiment.\n",
    "\n",
    "You will see the date and name of `runs` and the metadata information (input parameters, metrics, tags) logged for each `run`:\n",
    "\n",
    "You will also see a list of `runs` with metrics showing how your model performed with each set of parameters. Each line in the table represents once of the time you trained the model.\n",
    "\n",
    "<font color=\"green\"> >Note: You may see a series of MLflow `runs` that have been run in the past and sorted in descendant order (from the newest to the oldest). Your `runs` are the most recent ones. </font>  \n",
    "\n",
    "![MLFlow UI visualization of the runs](MLflow-UI-All-Runs.png)\n",
    "\n",
    "In this part of the lab, you have trained the model twice. The first time from your local Jupyter Notebook with _learning_rate=0.15_ and _max_depth=1_, and the second time from the ML Ops remote training cluster with _learning_rate=0.15_ and _max_depth=3_. Both the models got registered under your Experiment. \n",
    "\n",
    "You can easily recognized each of your `runs` based on the **most recent date** (column Start Time), their **name** (column Run Name) and their **source** (column Source).\n",
    "\n",
    "* The source of a model developed and trained in the ML Ops Jupyter Notebook is the following: ***ipykernel_launch*** with user being your ***Student\\<ID\\>*** \n",
    "\n",
    "* The source of a model developed and trained in the ML Ops remote training cluster is the following: ***train\\<xx\\>*** with user being ***root***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine the best performing model\n",
    "\n",
    "Using the MLFlow tracking server UI, you can easily determine the model that yields to the best prediction result by looking at the ***Metrics:*** (training score and evaluation rmse) logged in MLflow for each `run`. \n",
    "\n",
    "You can visualize the models in the MLflow tracking server UI and determine the best performing model based on the following criteria: \n",
    "\n",
    "* For RMSE the lower the value is, the better the fit.\n",
    "\n",
    "* For the model training score, the closer towards 1, the better the fit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6- Fetch the MLflow model artifact URI** \n",
    "\n",
    "Once you have identified your best performing model, you are ready to operationalize it by moving it to production for model serving.\n",
    "\n",
    "In order to deploy a model into production and serve predictions, you first need to get the ***URI of the MLflow model artifact*** that is stored of the MinIO S3 bucket on the MLflow tracking server. \n",
    "\n",
    "You can do this using either the MLflow tracking server UI or using a programmatic approach with the MLflow tracking Python API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the MLflow tracking server UI:\n",
    "\n",
    "If you **click on the date of the run** for your best performing model (this is the `run` with owner \"root\" and source \"Train\\<xx\\>\"), you will get the complete details of the metadata information (parameters, metrics, tags) and model artifact for that particular `run`. \n",
    "\n",
    "At the top, MLflow shows the name of the `run` and its metadata information. To find the model artifact URI, scroll down to the **Artifacts** section below the metadata information in the selected `run`. \n",
    "\n",
    "![image.png](MLflow-UI-Model-Artifacts.png)\n",
    "\n",
    "You can see the artifacts generated by the `run`:\n",
    "* an ML model file with metadata that allows MLflow to run the model.\n",
    "* a conda.yaml file that can be used for conda environment (a packaged and environment management system). \n",
    "* a serialized version of the model: ***model.pkl***. This is pickle format of the model that is saved in a file that you can use to deploy the model into an HPE Ezmeral ML Ops model serving endpoint.\n",
    "\n",
    "To get the Model artifact URI, expand the **model** field, **select and copy (CTRL+C)** the ***Full Path*** string value to **your clipboard**. You will need it in the next part of the lab to register the model in the MLflow registry of HPE Ezmeral ML Ops. \n",
    "\n",
    ">**Note:** _Do not click the Register Model button in the MLflow tracking server UI. You must register the model through the MLflow registry of HPE Ezmeral ML Ops. You will register your model in the next part of the lab_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a programmatic approach:\n",
    "\n",
    "An alternative way to interact with MLflow tracking server is using its API. As shown in the Python code cell below, a programmatic approach is used to determine the best performing model and get the MLflow model artifact URI. The MLflow tracking API for Python is used here.\n",
    "\n",
    "The following API calls to query the MLflow tracking and fetch metadata information about the `runs` from your active Experiment:\n",
    "\n",
    "* **search_runs()** – Query metadata information from logged `runs` and return the information that fits the search criteria in a dataframe (a tabular data structure).\n",
    "* **get_run()** - Fetch metadata information for a particular `run`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Search for the best model\n",
    "print(\"Best model information:\")\n",
    "best_run_df = mlflow.search_runs(TeamExperimentId,max_results=1,order_by=[\"metrics.eval_rmse ASC\"])\n",
    "print(best_run_df[[\"metrics.eval_rmse\", \"tags.mlflow.runName\", \"run_id\"]])\n",
    "print(\"\")\n",
    "best_run = mlflow.get_run(best_run_df.at[0, 'run_id'])\n",
    "best_model_uri = f\"{best_run.info.artifact_uri}/model\"\n",
    "# print best run info\n",
    "#print(f\"Run id: {best_run.info.run_id}\")\n",
    "#print(f\"Run parameters: {best_run.data.params}\")\n",
    "#print(\"Run score: RMSE = {:.4f}\\n\\n\".format(best_run.data.metrics['eval_rmse']))\n",
    "print(\"\")\n",
    "print(f\"Your best model artifact URI: {best_model_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select and copy (CTRL+C)** the model artifact URI above to **your clipboard**. You will need it in the next part of the lab to register the model in the MLflow registry of HPE Ezmeral ML Ops. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7- Model registry and deployment**\n",
    "\n",
    "You are now ready to register your best performing trained model in the MLflow registry of HPE Ezmeral ML Ops, and deploy it to production to serve predictions. \n",
    "\n",
    "So let's continue with the lab part 4.\n",
    "\n",
    "<font color=\"red\">**Go back to your JupyterHub account session to continue the hands-on from Lab 4 for model registry and model deployment:**</font>\n",
    "<font color=\"blue\">**4-WKSHP-MLOps-MLflow-Register-Model-Deployment.ipynb**.</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**======================================================================================**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
